{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7bd19c094dc44574d64e00d65102e6ba36e1076719a93266f721ddb81f822a22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning Course - LAB 1\n",
    "\n",
    "## Intro to PyTorch\n",
    "\n",
    "PyTorch (PT) is a Python (and C++) library for Machine Learning (ML) particularly suited for Neural Networks and their applications.\n",
    "\n",
    "Its great selection of built-in modules, models, functions, CUDA capability, tensor arithmetic support and automatic differentiation functionality make it one of the most used scientific libraries for Deep Learning.\n",
    "\n",
    "Note: for this series of labs, we advise to install Python >= 3.7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "We advise to install PyTorch following the directions given in its [home page](https://pytorch.org/get-started/locally/). Just typing `pip install torch` may not be the correct action as you have to take into account the compatibility with `cuda`. If you have `cuda` installed, you can find your version by typing `nvcc --version` in a terminal (Linux/iOS). \n",
    "\n",
    "If you're using Windows, we first suggest first to install Anaconda and then install PyTorch from the `anaconda prompt` software via `conda` (preferably) or `pip`.\n",
    "\n",
    "If you're using Google Colab, all the libraries needed to follow this lecture should be pre-installed there.\n",
    "\n",
    "We see now how to operate on Colab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### For Colab users\n",
    "\n",
    "Google Colab is a handy tool that we suggest you use for this course---especially if your laptop does not support CUDA or has limited hardware capabilities. Anyway, note that **we'll try to avoid GPU code as much as possible**.\n",
    "\n",
    "Essentially, Colab renders available to you a virtual machine with a limited hardware capability and disk where you can execute your code inside a given time window. You can even ask for a GPU (if you use it too much you'll need to start waiting a lot before it's available though)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Your (maybe) first Colab commands\n",
    "\n",
    "Colab Jupyter-style notebook interface with a few tweaks.\n",
    "\n",
    "For instance, you may run (some) bash command from here prepending `!` to your code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mnzluca/IntroToAI"
   ]
  },
  {
   "source": [
    "This makes it very easy to operate your virtual machine without the need for a terminal. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### File transfer on Colab\n",
    "\n",
    "One of the most intricate action in Colab is file transfer. Since your files reside on the virtual machine, there're two main ways to operate file transfer on Colab.\n",
    "\n",
    "* `files.download()` / `.upload()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"sample_data/README.md\")"
   ]
  },
  {
   "source": [
    "Although it may be much more handy to connect your Google Drive to Colab. Here is a snippet that lets you do this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "folder_mount = '/content/drive' # Your Drive will be mounted on top of this path\n",
    "\n",
    "drive.mount(folder_mount)"
   ]
  },
  {
   "source": [
    "### Dive into PyTorch - connections with NumPy\n",
    "\n",
    "Like NumPy, PyTorch provides its own multidimensional array class, called `Tensor`. `Tensor`s are essentially the equivalent of NumPy `ndarray`s.\n",
    "If we wish to operate a very superficial comparison between `Tensor` and `ndarray`, we can say that:\n",
    "* `Tensor` draws a lot of methods from NumPy, although it's missing some (see [this GitHub issue](if you're interested))\n",
    "* `Tensor` is more OO than `ndarray` and solves some inconsistencies within NumPy\n",
    "* `Tensor` has CUDA support"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# create custom Tensor and ndarray\n",
    "x = torch.Tensor([[1,5,4],[3,2,1]])\n",
    "y = np.array([[1,5,4],[3,2,1]])\n",
    "\n",
    "def pretty_print(obj, title=None):\n",
    "    if title is not None:\n",
    "        print(title)\n",
    "    print(obj)\n",
    "    print(\"\\n\")\n",
    "\n",
    "pretty_print(x, \"x\")\n",
    "pretty_print(y, \"y\")"
   ]
  },
  {
   "source": [
    "What are the types of these objs?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "source": [
    "`torch` already thinks with Machine Learning in mind as the `Tensor` is implicitly converted to `dtype float32`, while NumPy makes no such assumption.\n",
    "\n",
    "For more info on `Tensor` data types, please check the beginning of [this page](https://pytorch.org/docs/stable/tensors.html).\n",
    "\n",
    "As in NumPy, we can call the `.shape` attribute to get the shape of the structures. Moreover, `Tensor`s have also the `.size()` method which is analogous to `.shape`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape, x.size()"
   ]
  },
  {
   "source": [
    "Notice how a `Tensor` shape is **not** a tuple.\n",
    "\n",
    "We can also create a random `Tensor` analogously to NumPy.\n",
    "\n",
    "A `2 × 3 × 3` `Tensor` is the same as saying \"2 3 × 3 matrices\", or a \"cubic matrix\"\n",
    "\n",
    "![](img/tensors.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([2, 3, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.rand(2, 3, 3)\n",
    "y"
   ]
  },
  {
   "source": [
    "We can get the total number of elements in a `Tensor` via the `numel()` method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel()"
   ]
  },
  {
   "source": [
    "We can get the memory occupied by each element of a `Tensor` via `element_size()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.element_size()"
   ]
  },
  {
   "source": [
    "Hence, we can quickly calculate the size of the `Tensor` within the RAM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel() * x.element_size()"
   ]
  },
  {
   "source": [
    "#### Slicing a `Tensor`\n",
    "\n",
    "You can slice a `Tensor` (*i.e.*, extract a substructure of a `Tensor`) as in NumPy using the square brackets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first element (i.e., matrix) of first dimension\n",
    "pretty_print(x[0], \"Slice first element (x[0])\")\n",
    "\n",
    "# extract a specific element\n",
    "pretty_print(x[1,2,0], \"Slice element at (1, 2, 0) (x[1, 2, 0])\")\n",
    "\n",
    "# extract first element of second dimension (\":\" means all the elements of the given dim)\n",
    "pretty_print(x[:, 0], \"Slice first element of second dim (x[:, 0])\")\n",
    "\n",
    "# note that it is equivalent to\n",
    "pretty_print(x[:, 0, :], \"As above (x[:, 0] equivalent to x[:, 0, :])\")\n",
    "\n",
    "# extract range of dimensions (first and second element of third dim) \n",
    "pretty_print(x[:, :, 0:2], \"Slice first and second el of third dim (x[:, :, 0:2])\")\n",
    "\n",
    "# note that it is equivalent to (i.e., you can also pass list for slicing, as opposed to Py vanilla lists/tuples)\n",
    "pretty_print(x[:, :, (0, 1)], \"As above (x[:, :, 0:2] equivalent to x[:, :, (0, 1)])\")"
   ]
  },
  {
   "source": [
    "In Py, you can also slice any list by interval via the \"double colon\" notation `::` (`from`:`to - 1`:`step`). Note that `::3` means \"take all elements of the object by step of 3 starting from 0 until the list ends\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.range(0, 10)[0:7:3]"
   ]
  },
  {
   "source": [
    "#### `Tensor` supports linear algebra"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.rand([4, 5])\n",
    "print(\"z1\")\n",
    "print(\"shape\", z1.shape)\n",
    "print(z1)\n",
    "\n",
    "# transposition\n",
    "z2 = z1.T\n",
    "\n",
    "print(\"\\nz2\")\n",
    "print(\"shape\", z2.shape)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "pretty_print(z1 @ z2, \"Matrix multiplication: with '@'\")\n",
    "\n",
    "# equivalent to\n",
    "pretty_print(torch.matmul(z1, z2), \"Matrix multiplication: with torch.matmul\")\n",
    "\n",
    "# and also\n",
    "pretty_print(z1.matmul(z2), \"Matrix multiplication: with Tensor.matmul\")"
   ]
  },
  {
   "source": [
    "Note that `@` identifies the matrix product.\n",
    "\n",
    "Don't mistake `@` and `*` as the latter is the Hadamard (element-by-element) product!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 * z2 # this gives an Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 * z1"
   ]
  },
  {
   "source": [
    "Generally, the \"regular\" arithmetic operators for Python act as element-wise operators in `Tensor`s (as in `ndarrays`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 ** 2 # Equivalent to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3 = torch.Tensor([[1,2,3,4,7],[0.2,2,4,5,3],[-1,3,-4,2,2],[1,1,1,1,2]])\n",
    "pretty_print(z1 % z3, \"z1 % z3 (remainder of integer division)\")\n",
    "pretty_print(z3 // z1, \"z3 // z1 (integer division)\") # integer division\n",
    "z3 /= z1\n",
    "pretty_print(z3, \"in-place tensor division (z3 /= z1)\")"
   ]
  },
  {
   "source": [
    "As for `ndarrays`, `Tensor`s arithmetic operations support **broadcasting**. Roughly speaking, when two `Tensor`s have different shapes and a binary+ operator is applied to them, PT will try to find a way to make these objects \"compatible\" for the operation. \n",
    "\n",
    "Of course, broadcasting is not always possible, but as a rule of thumb, if some dimensions of a `Tensor` are one and the other dimensions are the same, broadcasting works."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_vector_5 = torch.Tensor([1,2,3,5,2]) # this is treated as a row vector (1 x 5 matrix)\n",
    "print(\"small_vector_5:\", small_vector_5, \"; Shape:\", small_vector_5.shape, \"\\n\")\n",
    "\n",
    "pretty_print(z1 / small_vector_5, \"Broadcasting: dividing matrix by row vector\")\n",
    "\n",
    "small_vector_4 = torch.Tensor([4,2,3,1])\n",
    "small_vector_4 = small_vector_4.unsqueeze(-1) # this operation \"transposes\" the vector into a column vector (4 x 1 matrix)\n",
    "print(\"small_vector_4:\\n\", small_vector_4, \"\\nShape:\", small_vector_4.shape, \"\\n\")\n",
    "\n",
    "pretty_print(z1 / small_vector_4, \"Broadcasting: dividing matrix by column vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([1,2,3]) == torch.Tensor([[1,2,3]]) # single-dim Tensors are also row vectors"
   ]
  },
  {
   "source": [
    "#### Reshaping and permuting\n",
    "\n",
    "Sometimes it may be necessary to reshape the tensors to apply some specific operators.\n",
    "\n",
    "Take the example of RGB images: they can be seen as `3 x h x w` tensors, where `h` is the height and `w` the width.\n",
    "\n",
    "![](img/image_tensor.png)\n",
    "\n",
    "Sometimes, it may be necessary to \"flatten\" the three matrices into vectors, thus obtaining a `3 x hw` tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.load(\"data/img.pt\")\n",
    "image.shape"
   ]
  },
  {
   "source": [
    "This flattening may be achieved via the `reshape` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reshaped = image.reshape(3, 243*880)\n",
    "pretty_print(image_reshaped.shape, \"shape of image_reshaped\")"
   ]
  },
  {
   "source": [
    "We can alternatively use the `view` method..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_view = image.view(3, 243*880)\n",
    "pretty_print(image_view.shape, \"shape of image_view\")"
   ]
  },
  {
   "source": [
    "**Q**: what is the difference between `reshape` and `view`?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Some libraries encode images as `h x w x 3` tensors instead of `3 x h x w`.\n",
    "\n",
    "To convert between these two format, one may be tempted to `reshape` or `view` the tensor: in the end, they share the number of elements.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image2 = image.reshape(243, 880, 3)\n",
    "\n",
    "plt.imshow(image2)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "That does not seem to work though: reshape does not change the order of the elements within the memory.\n",
    "\n",
    "In order to do so, we need to use `permute`, which changes shape **and** the order of the elements.\n",
    "We need to pass the new order of the dimensions to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = image.permute(1,2,0) # 1,2,0 --> old dim1 goes first, old dim2 goes second, dim0 goes last\n",
    "# can also do image.permute(-2,-1,0) -- works with negative indices as well\n",
    "plt.imshow(image3)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We already saw a case of incompatible `Tensor`s above. Which one is it?\n",
    "\n",
    "Some more lineal algebra...\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3_norm = z3.norm(2)\n",
    "pretty_print(z3_norm, \"Tensor norm\")\n",
    "pretty_print(np.linalg.norm(y), \"ndarray norm\") # notice how torch is more OO"
   ]
  },
  {
   "source": [
    "Notice how methods reducing `Tensor`s to scalars still return singleton `Tensor`s. (be wary of this feature when scripting something in PT)\n",
    "\n",
    "To \"disentangle\" the scalar from a `Tensor` use the `.item()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3_norm.item()"
   ]
  },
  {
   "source": [
    "Note that, as for NumPy, PT supports `Tensor`s operator on a subset of its dimensions.\n",
    "\n",
    "For example, given a `3x4x4 Tensor`, we might want to calculate the norm of each of the three `4x4` matrices composing it. We must hence specify to the `norm` method the dimensions on which we want it to operate the reduction:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z4 = torch.rand((3,4,4))\n",
    "pretty_print(z4.norm(dim=(1,2)), \"Norm of the three matrices composing z4 -- z4.norm(dim=(1,2))\")"
   ]
  },
  {
   "source": [
    "As expected, the result is a `1x3 Tensor`, showing the norm of each of the matrices.\n",
    "\n",
    "We can notice this behaviour in other `Tensor` operator applying a reduction:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z4)\n",
    "print(z4[0,:,0])\n",
    "print(z4[0,:,0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(z4.sum(dim=0), \"z4.sum(dim=0) -- Sum of the three matrices composing the tensor -- is a 4x4 matrix\")\n",
    "\n",
    "pretty_print(z4.prod(dim=1), \"z4.prod(dim=1) -- Product of the columns of each matrix composing the tensor\")"
   ]
  },
  {
   "source": [
    "pretty_print(z4[0, :, 0], \"This is how the first element of z4.prod(dim=1) is obtained. We fix the 1st and 3rd dim to `0` ...\")\n",
    "pretty_print(z4[0, :, 0].prod(), \"... and we operate the prod on it\")\n",
    "print(\"The other elements are obtained by looping through all of the other possible 1st and 3rd dimension indices. For instance, the element `2,3` of the reduction is obtained as\\nz4[2, :, 3].prod().\\n\")\n",
    "pretty_print(z4[2, : ,3], \"z4[2, : ,3]\")\n",
    "pretty_print(z4[2, :, 3].prod(), \"z4[2, : ,3].prod()\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### Seamless conversion from NumPy to PT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = torch.from_numpy(y)\n",
    "pretty_print(y_torch, \"y converted to torch.Tensor\")\n",
    "\n",
    "x_numpy = x.numpy()\n",
    "pretty_print(x_numpy, \"x converted to numpy.ndarray\")\n",
    "\n",
    "# Note that NumPy implicitly converts Tensor to ndarray whenever it can; the same doesn't happen for PT\n",
    "pretty_print(np.linalg.norm(x), \"Example of implicit conversion Tensor → ndarray\")"
   ]
  },
  {
   "source": [
    "#### Stochastic functionalities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can render the (pseudo)random number generator deterministic by calling `torch.manual_seed(integer)`.\n",
    "\n",
    "This works for both CPU and CUDA RNG calls."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123456)\n",
    "print(\"...from now on our random tensor should be the same...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(torch.randperm(10), \"(randperm) Random permutation of 0:10\")\n",
    "\n",
    "pretty_print(torch.rand_like(z1), \"(rand_like) Create random vector with the same shape of z_1\")\n",
    "\n",
    "pretty_print(torch.randint(10, (3, 3)), \"(randint) Like rand, but with integers up to 10\")\n",
    "\n",
    "pretty_print(torch.normal(0, 1, (3, 3)), \"(normal) Sampling a 3x3 iid scalars from N(0,1)\")\n",
    "\n",
    "pretty_print(torch.normal(torch.Tensor([[1,2,3],[4,5,6],[0,0,0]]), torch.Tensor([[1,0.5,0.9],[0.5,1,0.1],[3,4,1]])), \"Sampling from 9 normals with different means and std into a (3x3) Tensor\")"
   ]
  },
  {
   "source": [
    "### Using GPUs\n",
    "\n",
    "All `Torch.Tensor` methods support GPU computation via built-in CUDA wrappers.\n",
    "\n",
    "Just transfer the involved `Tensor`s to CUDA and let the magic happen :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available on this machine\n",
    "torch.cuda.is_available()\n",
    "\n",
    "has_cuda_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_cuda_gpu:\n",
    "    dim = 10000\n",
    "    large_cpu_matrix = torch.rand((dim, dim)) \n",
    "    large_gpu_matrix = large_cpu_matrix.to(\"cuda\") # Can also specify \"cuda:gpu_id\" if multiple GPUs\n",
    "    # alternatively, you may also call large_cpu_matrix.cuda() or large_cpu_matrix.cuda(0)\n",
    "else:\n",
    "    print(\"Sorry, this part of the notebook is inaccessible since it seems you don't have a CUDA-capable GPU on your device :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(large_cpu_matrix.device, \"Device of large_cpu_matrix\")\n",
    "pretty_print(large_gpu_matrix.device, \"Device of large_gpu_matrix\")\n",
    "pretty_print(large_gpu_matrix, \"If a tensor is not on CPU, the device will also be printed if you print the tensor itself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_cuda_gpu:\n",
    "    import timeit\n",
    "\n",
    "    # NOTE: please fix this number w.r.t. your GPU and CPU\n",
    "    repetitions = 100\n",
    "\n",
    "    print(\"Norm of large cpu matrix. Time:\", timeit.timeit(\"large_cpu_matrix.norm()\", number=repetitions, globals=locals()))\n",
    "    print(\"Norm of large gpu matrix. Time:\", timeit.timeit(\"large_gpu_matrix.norm()\", number=repetitions, globals=locals()))\n",
    "else:\n",
    "    print(\"Sorry, this part of the notebook is inaccessible since it seems you don't have a CUDA-capable GPU on your device :/\")"
   ]
  },
  {
   "source": [
    "Captain obvious: Use `tensor.cpu()` or `tensor.to(\"cpu\")` to move a tensor to your CPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building easy ML models\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first ML model using PyTorch: a linear regressor, whose model is\n",
    "\n",
    "`y = XW + b`\n",
    "\n",
    "which can also be simplified as\n",
    "\n",
    "`y = XW`\n",
    "\n",
    "if we incorporate the bias `b` inside `W` and add to the `X` a column of ones to the right.\n",
    "\n",
    "We'll first create our data. The `X`s are the 0:9 range plus some iid random noise, while the `y` is just the 0:9 range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.range(0, 9).unsqueeze(-1)\n",
    "x2 = torch.range(0, 9).unsqueeze(-1)\n",
    "x3 = torch.range(0, 9).unsqueeze(-1)\n",
    "x0 = torch.ones([10]).unsqueeze(-1)\n",
    "X = torch.cat((x1, x2, x3), dim=1)\n",
    "eps = torch.normal(0, .3, (10, 3))\n",
    "X += eps\n",
    "X = torch.cat((X, x0), dim=1)\n",
    "\n",
    "y = torch.range(0, 9).unsqueeze(-1)\n",
    "\n",
    "\n",
    "pretty_print(X, \"X (covariates)\")\n",
    "pretty_print(y, \"y (response)\")"
   ]
  },
  {
   "source": [
    "For the case of linear regression, we usually wish to obtain a set of weights minimizing the so called mean square error/loss (MSE), which is the squared difference between the ground truth and the model prediction, summed for each data instance.\n",
    "\n",
    "We know that the OLS/Max Likelihood esitmator is the one yielding the optimal set of weights in that regard."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = ((X.T @ X).inverse()) @ X.T @ y # OLS estimator\n",
    "\n",
    "pretty_print(W_hat, \"W (weights - coefficients and bias/intercept)\")"
   ]
  },
  {
   "source": [
    "We can evaluate our model on the mean square loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_loss(y, y_hat):\n",
    "    return (((y - y_hat).norm())**2).item() / y.shape[0]"
   ]
  },
  {
   "source": [
    "Let's apply it to our data.\n",
    "\n",
    "First we need to obtain the predictions, then we can evaluate the MSE."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = X @ W_hat\n",
    "pretty_print(y_hat, \"Predictions (y_hat)\")\n",
    "\n",
    "pretty_print(mean_square_loss(y, y_hat), \"Loss (MSE)\")"
   ]
  },
  {
   "source": [
    "#### Using PT built-ins\n",
    "\n",
    "We will now be exploring the second chunk of PT functionalities, namely the built-in structures and routines supporting the creation of ML models.\n",
    "\n",
    "We can create the same model we have seen before using PT built-in structures, so we start to see them right away.\n",
    "\n",
    "Usually, a PT model is a `class` inheriting from `torch.nn.Module`. Inside this class, we'll define two methods:\n",
    "* the constructor (`__init__`) in which we define the building blocks of our model as class variables (later during our lectures we'll see more \"elegant\" methods to build models architectures)\n",
    "* the `forward` method, which specifies as the data fed into the model needs to be processed in order to produce the output\n",
    "\n",
    "Note for those who already know something about NNs: we don't need to define `backward` methods since we're constructing our model with built-in PT building blocks. PT automatically creates a `backward` routine based upon the `forward` method.\n",
    "\n",
    "Our model only has one building block (layer) which is a `Linear` layer.\n",
    "We need to specify the size of the input (i.e. the coefficients `W` of our linear regressor) and the size of the output (i.e. how many scalars it produces) of the layer. We additionaly request our layer to has a bias term `b` (which acts as the intercept of the line we saw before).\n",
    "\n",
    "The `Linear` layer processes its input as `XW + b`, which is exactly the (first) equation of the linear regressor we saw before.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regressor = torch.nn.Linear(in_features=3, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.regressor(X)"
   ]
  },
  {
   "source": [
    "We can create an instance of our model and inspect the current parameters by using the `state_dict` method, which prints the building blocks of our model and their current parameters. Note that `state_dict` is essentially a dictonary indexed by the names of the building blocks which we defined inside the constructor (plus some additional identifiers if a layer has more than one set of parameters)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegressor()\n",
    "\n",
    "for param_name, param in lin_reg.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "source": [
    "We can update the parameters via `state_dict` and re-using the same OLS estimates we obtained before.\n",
    "\n",
    "Note that PT is thought of for Deep Learning: it does not have (I think) the routines to solve different ML problems.\n",
    "\n",
    "Next time, we'll see as we can unleash PT's gradient-based iterative training routines and compare the results w.r.t. the OLS estimators."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = lin_reg.state_dict()\n",
    "state_dict[\"regressor.weight\"] = W_hat[:3].T\n",
    "state_dict[\"regressor.bias\"] = W_hat[3]\n",
    "lin_reg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it worked\n",
    "for param_name, param in lin_reg.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "source": [
    "The `forward` method gets implicitly called by passing the data to our model's instance `lin_reg`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg = X[:,:3]\n",
    "predictions_lin_reg = lin_reg(X_lin_reg)\n",
    "pretty_print(predictions_lin_reg, \"Predictions of torch class\")"
   ]
  },
  {
   "source": [
    "The predictions are the same as before"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(y_hat, \"Predictions of linear model\")"
   ]
  },
  {
   "source": [
    "### Adding non-linearity\n",
    "\n",
    "One of the staples of DL is that the relationship between the `X`s and the predictions is **non-linear**.\n",
    "\n",
    "The non-linearity is obtain by applying a non-linear function (called *activation function*) after each linear layer.\n",
    "\n",
    "We can complicate just a little bit our linear model to create a **logistic regressor**:\n",
    "\n",
    "`y = logistic(XW + b)`,\n",
    "\n",
    "where `logistic(z) = exp(z) / (1 + exp(z))`\n",
    "\n",
    "The logistic function has different names:\n",
    "* in statistics, it's usually called *inverse logit* as well\n",
    "* in DL and mathematics, it's called *sigmoid function* due to its \"S\" shape\n",
    "\n",
    "Hystorically, the sigmoid was between the first activation functions used in NNs.\n",
    "\n",
    "![](img/sigmoid.png)\n",
    "\n",
    "Logistic regression is usually used as a **binary classification model** instead of a regression model.\n",
    "In this setting, we suppose we have two destination classes to which we assign values 0 and 1: `y ∈ {0, 1}`.\n",
    "Since the codomain of the sigmoid is `[0,1]`, we can interpret its output `ŷ` as a probability value, and assign each data to the class 0 if `ŷ <= 0.5`, to the class 1 otherwise.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([0,1,0,0,1,1,1,0,1,1])\n",
    "pretty_print(y, \"y for our classification problem\")"
   ]
  },
  {
   "source": [
    "Note that we may also want our y to be a vector of `int`s.\n",
    "We can convert the `Tensor` type to `int` by calling the method `.long()` or `.int()` of `Tensor`.\n",
    "\n",
    "As in NumPy, the type of the `Tensor` is found within the `dtype` variable of the given `Tensor`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.int()\n",
    "pretty_print(y, \"y converted to int\")\n",
    "pretty_print(y.dtype, \"Data type of y\")"
   ]
  },
  {
   "source": [
    "Let us now build our logistic regressor in PT.\n",
    "\n",
    "We only need one single addition wrt the linear regressor: in the `forward` method, we'll add the sigmoidal non-linearity by calling the `sigmoid` function within the `torch.nn.functional` library.\n",
    "\n",
    "Note that there also exist some \"mirror\" alias of these functionals within `torch.nn` (e.g. `torch.nn.Sigmoid`): we'll learn in the following lecture why these aliases are there and how to use them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # no difference wrt linear regressor\n",
    "        self.regressor = torch.nn.Linear(in_features=3, out_features=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.regressor(X)\n",
    "        # here we apply the sigmoid fct to the output of regressor\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "We can instantiate our logistic regressor and use it to calculate our predictions on the same X as before.\n",
    "\n",
    "Note that **we're using the initial (random) weights which PT has assigned to the model parameters**.\n",
    "For our linear regressor, we were able to analytically obtain the OLS value of the parameters.\n",
    "In the case of logistic regression, there's no MaxLikelihood estimator obtainable in close form and we need to resort to numerical methods to obtain them.\n",
    "Since the part concerning numerical optimization will be discussed during the next Lab, we will not be training our model (hence results will obviously be sub-par)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegressor()\n",
    "y_hat = log_reg(X_lin_reg)\n",
    "pretty_print(y_hat, \"logistic regressor predictions\")"
   ]
  },
  {
   "source": [
    "There exist many ways to evaluate the performance of the logistic regressor: one of them is **accuracy** (correctly identified units / total number of units).\n",
    "We can define a function to evaluate accuracy and calculate it on our model and data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    # Assign each y_hat to its predicted class\n",
    "    pred_classes = torch.where(y_hat < .5, 0, 1).squeeze().int()\n",
    "    correct = (pred_classes == y).sum()\n",
    "    return (correct / y.shape[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y, y_hat)"
   ]
  },
  {
   "source": [
    "#### Visualizing linear and logistic regression as a computational graph\n",
    "\n",
    "We now need to convert the equation of the linear and the logistic regression:\n",
    "* `y = σ(WX + b)`\n",
    "\n",
    "where `σ` is a generic `ℝ → ℝ` function: sigmoid for logistic regression, identity for linear regression.\n",
    "\n",
    "![](img/log_reg_graph.jpg)\n",
    "\n",
    "We organize the input in *nodes* (on the left part) s.t. each node represents one dimension/covariate.\n",
    "For each data instance, we substitute to each node the corresponding numeric value.\n",
    "The nodes undergo one or more operations, namely, from left to right:\n",
    "\n",
    "1. Each node is multiplied by its corresponding weight (a value placed on the edge indicates that the node is multiplied by said value)\n",
    "2. All the corresponding outputs are summed together\n",
    "\n",
    "These two operations identify the dot product between vectors `X` and `W`\n",
    "\n",
    "3. The bias term `b` is added\n",
    "4. The non-linear function `σ` is applied to the result of this sum\n",
    "5. Finally, we assign that value to the variable `ŷ`, which is also indicated as a node\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Our first MultiLayer Perceptron (MLP)\n",
    "\n",
    "The MLP is a family of Artificial NNs in which the input is a vector of size `ℝ^d` and the output is again a vector of size `ℝ^p`, where `p` is determined upon the nature of the problem we wish to solve. Additionally, a MLP is characterized by multiple stages (*layers*) of sequential vector-matrix multiplication and non-linearity (steps 1., 2., 3. above) in which each output of the layer `l-1` acts as input to the layer `l`.\n",
    "\n",
    "Taking inspiration to the graph of the logistic regression, we can translate all into an image to give sense to these words:\n",
    "\n",
    "![](img/mlp_graph.jpg)\n",
    "\n",
    "In NNs, each of the nodes within the graph is called a **neuron**\n",
    "\n",
    "Neurons are organized in **layers**\n",
    "\n",
    "In computational graphs, layers are shown from left to right (or bottom to top sometimes), which is the direction of the flow of information inside the NN.\n",
    "\n",
    "The first layer is called **input layer** and represents the dimensions of our data.\n",
    "\n",
    "The last layer is called **output layer** and represent the output of our NN.\n",
    "\n",
    "All the intermediate layers are called **hidden layers**. To be defined MLP, there must be at least one hidden layer inside our model.\n",
    "\n",
    "If the NN is an MLP, each neuron in a given layer (except for the input) receives information from every neuron of the previous; moreover, each neuron in any layer (except for the output) sends information to every neuron of the next layer. There's no communication between neurons of the same layer (if it happens, we have a **Recursive Neural Network**)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the sake of brevity, usually in NN computational graphs we drop the blocks `+` and `σ`, the values of weights, and the reference to the bias terms, remaining with a scheme conveying info about\n",
    "* the number of neurons per layer\n",
    "* the connectivity of neurons\n",
    "\n",
    "The graph above becomes:\n",
    "\n",
    "![](img/mlp_graph_common.jpg)\n",
    "\n",
    "We can then start programming our simple MLP in PT.\n",
    "\n",
    "We will suppose that our MLP is for **binary classification**, hence the activation function `τ` is the sigmoid.\n",
    "\n",
    "**Q**: what if we wanted our MLP to operate a regression?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=3, out_features=2)\n",
    "        self.layer2 = torch.nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(X)\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "source": [
    "For the great majority of MLP, it's very hard to get analytical solutions to our sets of weights and biases. We then resort to numerical methods for optimization.\n",
    "\n",
    "In DL, we normally used gradient-based methods like Stochastic Gradient Descent with *backpropagation* to find approximate solutions.\n",
    "\n",
    "We'll cover these topics in future lectures. For now, the focus is to build a MLP in PT and perform the *forward pass* (=evaluate the model on a set of data)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can analyse the structure of our MLP by just printing the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "source": [
    "although we might wanna have additional informations.\n",
    "\n",
    "There's an additional package, called `torch-summary` which helps us producing more informative and exhaustive model summaries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch-summary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "source": [
    "Let us suppose we wish to build a larger model from the graph below.\n",
    "\n",
    "![](img/mlp_graph_larger.jpg)\n",
    "\n",
    "We suppose that\n",
    "\n",
    "1. The layers have no bias units\n",
    "2. The activation function for hidden layers is `ReLU`\n",
    "\n",
    "Moreover, we suppose that this is a classification problem.\n",
    "\n",
    "As you might recall, when the number of classes is > 2, we encode the problem in such a way that the output layer has a no. of neurons corresponding to the no. of classes. Doing so, we establish a correspondence between output units and classes. The value of the $j$-th neuron represents the **confidence** of the network in assigning a given data instance to the $j$-th class.\n",
    "\n",
    "Classically, when the network is encoded in such way, the activation function for the final layer is the **softmax** function.\n",
    "If $C$ is the total number of classes,\n",
    "\n",
    "$softmax(z_j) = \\frac{\\exp(z_j)}{\\sum_{k=1}^C \\exp(z_k)}$\n",
    "\n",
    "where $j\\in \\{1,\\cdots,C\\}$ is one of the classes.\n",
    "\n",
    "If we repeat this calculation for all $j$s, we end up with $C$ normalized values (i.e., between 0 and 1) which can be interpreted as probability that the network assigns the instance to the corresponding class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: build this network from scratch during class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and summarise"
   ]
  }
 ]
}