{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967",
   "display_name": "Python 3.8.8 64-bit ('lot': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 9\n",
    "\n",
    "## An explainability-first implementation of the Vision Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This lab will mainly follow the slides from the lecture on the Vision Transformer (ViT).\n",
    "\n",
    "Please refer to the slides for the methodological explanations.\n",
    "\n",
    "We will be constructing the ViT bottom-up, i.e. from the input embedding to the output.\n",
    "\n",
    "_**Disclaimer**: this lab is just for explanatory purposes. If you intend on using a ViT for your work, I suggest using one of the many plug-and-play versions you can find on the web (e.g. the one in `timm`), which are probably more efficient than this implementation. Here, I sacrifice performance for clarity._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms as T\n",
    "from scripts import mnistm, train"
   ]
  },
  {
   "source": [
    "## 1a. Patch + vectorize input\n",
    "\n",
    "The input is first subdivided into patches and each patch is *unrolled* into a 1D vector.\n",
    "\n",
    "Let us implement a generic torchvision-style transform which we can pass to a `Dataset`'s `transform` attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToVecPatch():\n",
    "    def __init__(self, patch_size, axis_channel=2):\n",
    "        '''\n",
    "        axis_channel is the axis (dim) in which the channels are located.\n",
    "        An image should be h x w x c -> 2\n",
    "        An image converted with ToTensor() should be c x h x w -> 0\n",
    "        '''\n",
    "        assert axis_channel in (0,2), f\"Method supports only axis_channel 0 or 2.\"\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.axis_channel = axis_channel\n",
    "\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        '''\n",
    "        sample is a torch.Tensor\n",
    "        '''\n",
    "        if self.axis_channel == 0:\n",
    "            sample = sample.permute(1,2,0)\n",
    "        \n",
    "        patch_vert = torch.cat(sample.split(self.patch_size[1], dim=1))\n",
    "        patch_horiz = torch.stack(patch_vert.split(self.patch_size[0], dim=0))\n",
    "        vec_patches = torch.flatten(patch_horiz, start_dim=1)\n",
    "        return vec_patches"
   ]
  },
  {
   "source": [
    "Let's see it in action on a small 4x4 grayscale image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1,2,3,4],[4,5,6,7],[7,8,9,0],[10,7,3,88]])\n",
    "patch_size = 2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_vert = torch.cat(x.split(patch_size, dim=1))\n",
    "patch_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_horiz = torch.stack(patch_vert.split(patch_size, dim=0))\n",
    "patch_horiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_patches = torch.flatten(patch_horiz, start_dim=1)\n",
    "vec_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ToVecPatch(2)\n",
    "P(sample=x)"
   ]
  },
  {
   "source": [
    "# 1b. Input embedding\n",
    "\n",
    "Now we need to take care of the input embedding:\n",
    "* we have an input $I$ with shape $N \\times P^2\\cdot c$, where:\n",
    "    * $N$ is the number of patches\n",
    "    * $P$ is the patch size\n",
    "    * $c$ is the channel size (1 in the example above)\n",
    "* we need to linearly project $I$ into $z_0$, belonging in the $N \\times D$ space, where $D$ is (hopefully) smaller than $P^2\\cdot c$\n",
    "* we also need to prepend a learnable `<class>` token to $z_0$\n",
    "* and we need to sum the **positional embedding/encoding** to it\n",
    "\n",
    "Also, in a `Module`-like class, we need to take into account that the input will be 3-dimensional ($B \\times N \\times P^2\\cdot c$, $B$ being the batch size)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedInput(nn.Module):\n",
    "    def __init__(self, num_patches, patch_dim, latent_dim, bias=False, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(patch_dim, latent_dim, bias=bias) # this represents the matrix E\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # the next params are the same independent of the batch size\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, latent_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, latent_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = self.embed(X)\n",
    "        z = self.dropout(z)\n",
    "        z = torch.cat((self.class_token.expand(z.shape[0], *self.class_token.shape[1:]), z), dim=1)\n",
    "        z += self.pos_embedding\n",
    "        return z"
   ]
  },
  {
   "source": [
    "Let's try it on real data. We use MNISTM since it's an easy dataset, but the images have 3 channels unlike MNIST.\n",
    "\n",
    "Create the `Dataset` and pull one batch of data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4639, 0.4676, 0.4199], [0.2534, 0.2380, 0.2618]),\n",
    "    ToVecPatch(7, axis_channel=0), # image of size 28*28, patches of size 7\n",
    "])\n",
    "\n",
    "mnistm_train = mnistm.MNISTM(root=\"datasets/MNISTM\", download=True, transform=transforms)\n",
    "dataloader = torch.utils.data.DataLoader(mnistm_train, 128, shuffle=False, num_workers=8)\n",
    "batch, _ = next(iter(dataloader))\n",
    "batch.shape"
   ]
  },
  {
   "source": [
    "embed the batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedInput(num_patches=16, patch_dim=147, latent_dim=24)\n",
    "embedded = embed(batch)\n",
    "embedded.shape"
   ]
  },
  {
   "source": [
    "## 2. Attention\n",
    "\n",
    "* We have an embedded input $z_0$ of shape $B\\times (N+1)\\times D$\n",
    "\n",
    "* We need to:\n",
    "    * get $Q, K, V \\in \\mathbb{R}^{B\\times (N+1)\\times d}$ through linear projection from $z_0$\n",
    "    * obtain $A = \\text{softmax}(QK^\\top/\\sqrt{d})$\n",
    "    * get $S = AV$\n",
    "\n",
    "all this for each head $h\\in\\{1,\\dots H\\}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, input_dim, attention_dim, bias=False, dropout_p=0.0):\n",
    "        '''\n",
    "        input_dim -> D\n",
    "        attention_dim -> d\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.u_qkv = nn.Linear(input_dim, attention_dim * num_heads * 3, bias=bias)\n",
    "        self.u_msa = nn.Linear(attention_dim * num_heads, input_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        QKV = self.u_qkv(z).chunk(3, dim=-1)\n",
    "        separate_heads = lambda tensor: tensor.reshape(*tensor.shape[:2], self.num_heads, self.attention_dim).permute(0,2,1,3)\n",
    "        Q, K, V = [separate_heads(t) for t in QKV]\n",
    "        '''\n",
    "        Why all that mess?\n",
    "            Out of the linear projection we get a tensor of shape B x (N+1) x 3Hd\n",
    "            We separate this tensor into three chunks of shape B x (N+1) x Hd\n",
    "            We now need to \"enucleate\" the head from the third dim (->reshape)\n",
    "            Then, for simplicity, we shift the head to the second dim (->permute)\n",
    "            Shape: B x H x (N+1) x d\n",
    "            Now, for each head, we need to do the dot product between Q and K\n",
    "            This can be done in an elegant way using the einstein notation (einsum)\n",
    "        '''\n",
    "        A = torch.einsum(\"b h n d, b h m d -> b h n m\", Q, K) / (self.attention_dim ** .5)\n",
    "        '''\n",
    "        We can use only small letters (no capitals)\n",
    "        b is batch size, h is head size, d is attention_dim\n",
    "        n and m are the no. of patches for Q and K respectively\n",
    "        Despite being =, we must name them differently so torch knows\n",
    "        how to carry out the product\n",
    "        '''\n",
    "        A = torch.nn.functional.softmax(A, dim=-1)\n",
    "        S = torch.einsum(\"b h n m, b h m d -> b h n d\", A, V)\n",
    "        # undo separate_heads\n",
    "        S = S.permute(0, 2, 1, 3)\n",
    "        S = S.reshape(*S.shape[:2], S.shape[2]*S.shape[3])\n",
    "        S = self.u_msa(S)\n",
    "        return self.dropout(S)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = MultiheadedSelfAttention(num_heads=6, input_dim=24, attention_dim=7)\n",
    "z_prime = msa(embedded)\n",
    "z_prime.shape"
   ]
  },
  {
   "source": [
    "## 3. MLP layer\n",
    "\n",
    "Very easy, let's do it by ourselves...\n",
    "\n",
    "$(B\\times (N+1) \\times D) \\rightarrow (B\\times (N+1)\\times m) \\rightarrow (B\\times (N+1)\\times D)$ \n",
    "\n",
    "**add dropout wherever it's possible**\n",
    "\n",
    "**use `GeLU` non-linearity**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, bias=True, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim=24, hidden_dim=512)\n",
    "z_1 = mlp(z_prime)\n",
    "z_1.shape"
   ]
  },
  {
   "source": [
    "## 4. The MSA Layer\n",
    "\n",
    "We need to put together 2. and 3.\n",
    "\n",
    "![](img/msa_layer.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSALayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attention_dim, mlp_dim, bias_msa=False, bias_mlp=True):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.msa = MultiheadedSelfAttention(num_heads, embed_dim, attention_dim, bias=bias_msa)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, bias=bias_mlp)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # DIY\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msal = MSALayer(embed_dim=24, num_heads=6, attention_dim=7, mlp_dim=512)\n",
    "z = msal(embedded)\n",
    "z.shape"
   ]
  },
  {
   "source": [
    "## 5. The final MLP head\n",
    "\n",
    "Easy...\n",
    "\n",
    "$(B \\times D) \\rightarrow (B \\times \\kappa)$\n",
    "\n",
    "before, we use layernorm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, bias=True):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes, bias=bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.fc(self.layernorm(X))"
   ]
  },
  {
   "source": [
    "## Let's put all of our pieces together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        patch_dim,\n",
    "        embed_dim,\n",
    "        num_msa_layers,\n",
    "        num_heads,\n",
    "        attention_dim,\n",
    "        mlp_dim,\n",
    "        num_classes,\n",
    "        bias_embed=False,\n",
    "        bias_msa=False,\n",
    "        bias_mlp_att=True,\n",
    "        bias_mlp_head=True\n",
    "        # no dropout for simplicity\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_embedder = EmbedInput(num_patches, patch_dim, embed_dim, bias=bias_embed)\n",
    "        self.msa = nn.Sequential(\n",
    "            *([MSALayer(embed_dim, num_heads, attention_dim, mlp_dim, bias_msa=bias_msa, bias_mlp=bias_mlp_att)] * num_msa_layers)\n",
    "        )\n",
    "        self.head = MLPHead(embed_dim, num_classes, bias=bias_mlp_head)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X is already a tensor B images decomposed into vectorized patches\n",
    "        '''\n",
    "        out = self.input_embedder(X)\n",
    "        out = self.msa(out)\n",
    "        out = out[:,0] # keep only the \"context token\"\n",
    "        return self.head(out)\n"
   ]
  },
  {
   "source": [
    "Let's first build a small ViT with 4 MSA layers and the specs from our tries, then check whether the dimension of the output is as expected:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(num_patches=16, patch_dim=147, embed_dim=24, num_msa_layers=4, num_heads=6, attention_dim=7, mlp_dim=512, num_classes=10)\n",
    "y = vit(batch)\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "Let's try a quick 10-epochs training with default Adam. We shouldn't expect great performances as our model is very small and the transformer is thought for training on large-scale datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vit.parameters())\n",
    "train.train_model(vit, dataloader, nn.CrossEntropyLoss(), optimizer, 10)"
   ]
  },
  {
   "source": [
    "### Instantiate a ViT-Base model\n",
    "\n",
    "![](img/vit_models.jpg)\n",
    "\n",
    "Build it for images of size 224x224 and patches of size 16x16 (â†’196 patches).\n",
    "\n",
    "We comply with the paper and set $d=D/H=768/12=64$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(num_patches=196, patch_dim=16*16*3, embed_dim=768, num_msa_layers=12, num_heads=12, attention_dim=64, mlp_dim=3072, num_classes=1000)\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = summary(vit)"
   ]
  },
  {
   "source": [
    "Check the no. of parameters (note that the summary above doesn't recognize the sequences of `MSALayer`s for some reasons...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1536 * 2 + 2359296 + 4722432) * 12 + 1536 + 769000 + 196608"
   ]
  },
  {
   "source": [
    "This was just a demo showcasing one of the possible ways we can construct a structure like the Visual Transformers.\n",
    "\n",
    "If you need to use it, I suggest using pre-built stuff, like the one contained in `timm`.\n",
    "\n",
    "You'll notice that existing implementations tend to make more use of the `einops` library, which introduces some methods, ubiquitous to PyTorch and NumPy, for transposing (permuting) a tensor, repeating given dims... Check out the [docs](https://einops.rocks/1-einops-basics/) if you're interested."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}